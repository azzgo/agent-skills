#!/usr/bin/env python3
"""
Evaluation record format for LLM evaluation results.

Defines the JSON schema for evaluation records generated by the Evaluator instance.
See design.md lines 482-577 for format specification.
"""

import json
from dataclasses import dataclass, field, asdict
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path


@dataclass
class EvaluatorSession:
    """Metadata about the evaluator instance session."""

    instance_id: str
    model: str
    evaluation_time: str
    duration_ms: int


@dataclass
class CriteriaScores:
    """Scores for each evaluation criterion (0.0-1.0 scale)."""

    task_completion: float
    rpc_correctness: float
    reasoning_quality: float
    error_handling: float
    response_quality: float


@dataclass
class Judgment:
    """Overall judgment of the execution."""

    status: str  # "PASS" or "FAIL"
    confidence: float  # 0.0-1.0
    criteria_scores: CriteriaScores
    overall_score: float


@dataclass
class DecisionProcessAssessment:
    """Assessment of the LLM's decision-making process."""

    logical_flow: str  # "Excellent", "Good", "Fair", "Poor"
    tool_usage: str  # "Efficient", "Acceptable", "Inefficient"
    parameter_construction: str  # "Correct", "Partial", "Incorrect"
    error_handling: str  # "Excellent", "Good", "Fair", "Poor", "N/A"


@dataclass
class Analysis:
    """Detailed analysis of the execution."""

    strengths: List[str]
    weaknesses: List[str]
    decision_process_assessment: DecisionProcessAssessment


@dataclass
class FailureAnalysis:
    """Detailed analysis of failure cases."""

    root_cause: str
    location: Optional[str]
    error_trace: Optional[str]
    llm_context: str
    expected_behavior: str
    actual_behavior: str


@dataclass
class EvaluationRecord:
    """Complete evaluation record from the Evaluator instance."""

    test_id: str
    evaluator_session: EvaluatorSession
    judgment: Judgment
    analysis: Analysis
    improvement_suggestions: Optional[List[str]] = None
    failure_analysis: Optional[FailureAnalysis] = None
    priority: Optional[str] = None  # "HIGH", "MEDIUM", "LOW"
    recommended_actions: Optional[List[str]] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)

    def to_json(self) -> str:
        """Convert to JSON string."""
        return json.dumps(self.to_dict(), indent=2)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "EvaluationRecord":
        """Create EvaluationRecord from dictionary."""
        # Convert nested dicts to dataclasses
        data["evaluator_session"] = EvaluatorSession(**data["evaluator_session"])

        judgment_data = data["judgment"]
        judgment_data["criteria_scores"] = CriteriaScores(
            **judgment_data["criteria_scores"]
        )
        data["judgment"] = Judgment(**judgment_data)

        analysis_data = data["analysis"]
        analysis_data["decision_process_assessment"] = DecisionProcessAssessment(
            **analysis_data["decision_process_assessment"]
        )
        data["analysis"] = Analysis(**analysis_data)

        if data.get("failure_analysis"):
            data["failure_analysis"] = FailureAnalysis(**data["failure_analysis"])

        return cls(**data)

    @classmethod
    def from_json(cls, json_str: str) -> "EvaluationRecord":
        """Create EvaluationRecord from JSON string."""
        data = json.loads(json_str)
        return cls.from_dict(data)

    def save(self, output_dir: Path) -> Path:
        """
        Save evaluation record to a timestamped JSON file.

        Args:
            output_dir: Directory to save the record

        Returns:
            Path to the saved file
        """
        output_dir.mkdir(parents=True, exist_ok=True)

        # Generate filename: test_id_evaluation_timestamp.json
        timestamp = datetime.fromisoformat(
            self.evaluator_session.evaluation_time
        ).strftime("%Y%m%d_%H%M%S")
        filename = f"{self.test_id}_evaluation_{timestamp}.json"
        filepath = output_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(self.to_json())

        return filepath

    @classmethod
    def load(cls, filepath: Path) -> "EvaluationRecord":
        """
        Load evaluation record from a JSON file.

        Args:
            filepath: Path to the JSON file

        Returns:
            EvaluationRecord instance
        """
        with open(filepath, "r", encoding="utf-8") as f:
            return cls.from_json(f.read())


@dataclass
class TestResult:
    """Combined test result with both execution and evaluation."""

    test_id: str
    milestone: str
    test_name: str
    command: str
    execution_duration_ms: int
    evaluation_duration_ms: int
    status: str  # "PASS" or "FAIL"
    overall_score: float
    criteria_scores: CriteriaScores
    strengths: List[str]
    weaknesses: List[str]
    improvement_suggestions: Optional[List[str]] = None
    failure_analysis: Optional[FailureAnalysis] = None
    execution_record_path: Optional[str] = None
    evaluation_record_path: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)

    def to_json(self) -> str:
        """Convert to JSON string."""
        return json.dumps(self.to_dict(), indent=2)


def load_all_evaluations(
    directory: Path, filter_status: Optional[str] = None
) -> List[EvaluationRecord]:
    """
    Load all evaluation records from a directory.

    Args:
        directory: Directory containing evaluation record JSON files
        filter_status: Optional status filter ("PASS" or "FAIL")

    Returns:
        List of EvaluationRecord instances
    """
    records = []

    if not directory.exists():
        return records

    for filepath in directory.glob("*_evaluation_*.json"):
        try:
            record = EvaluationRecord.load(filepath)

            # Apply filter
            if filter_status and record.judgment.status != filter_status:
                continue

            records.append(record)
        except Exception as e:
            print(f"Warning: Failed to load {filepath}: {e}")

    return records


if __name__ == "__main__":
    # Example: Create a PASS evaluation record
    evaluation = EvaluationRecord(
        test_id="milestone1_add_uri_001",
        evaluator_session=EvaluatorSession(
            instance_id="opencode-eval-001",
            model="ZhipuAI/GLM-4.7",
            evaluation_time=datetime.now().isoformat(),
            duration_ms=2000,
        ),
        judgment=Judgment(
            status="PASS",
            confidence=0.95,
            criteria_scores=CriteriaScores(
                task_completion=1.0,
                rpc_correctness=1.0,
                reasoning_quality=0.9,
                error_handling=1.0,
                response_quality=0.95,
            ),
            overall_score=0.97,
        ),
        analysis=Analysis(
            strengths=[
                "Correctly identified user intent (download file)",
                "Matched intent to appropriate aria2 method (addUri)",
                "Constructed correct parameter format (array of URIs)",
            ],
            weaknesses=[
                "Could have provided more context about download progress monitoring"
            ],
            decision_process_assessment=DecisionProcessAssessment(
                logical_flow="Excellent",
                tool_usage="Efficient",
                parameter_construction="Correct",
                error_handling="N/A",
            ),
        ),
    )

    print(evaluation.to_json())
