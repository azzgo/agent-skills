# LLM Evaluation System Configuration
# This file configures the executor and evaluator instances for LLM evaluation tests

# Execution mode: 'simulated' or 'opencode'
# - simulated: Uses local simulation (fast, no external dependencies)
# - opencode: Calls real OpenCode API endpoints
execution_mode: simulated

# OpenCode Executor Configuration (Instance 1)
# This instance executes natural language commands with the aria2-json-rpc skill
executor:
  # Mode for executor: 'simulated' or 'opencode'
  # If not specified, uses global execution_mode
  mode: simulated
  
  # OpenCode API endpoint for executor instance
  # Example: 'http://localhost:8080' or 'https://opencode.example.com'
  api_endpoint: http://localhost:8080
  
  # Model configuration for executor
  model:
    # Model name/identifier (e.g., 'claude-3-5-sonnet-20241022', 'gpt-4', etc.)
    name: claude-3-5-sonnet-20241022
    # Optional model parameters
    temperature: 0.7
    max_tokens: 4096
  
  # Timeout for executor API calls (seconds)
  timeout: 60
  
  # Enable full LLM tracing to capture reasoning chain
  enable_tracing: true
  
  # Optional authentication for OpenCode API
  auth:
    # Authentication type: 'none', 'bearer', 'api_key'
    type: none
    # Token or API key (if required)
    token: ""
    # API key header name (if using api_key type)
    api_key_header: "X-API-Key"

# OpenCode Evaluator Configuration (Instance 2)
# This instance analyzes execution records and provides judgments
evaluator:
  # Mode for evaluator: 'simulated' or 'opencode'
  # If not specified, uses global execution_mode
  mode: simulated
  
  # OpenCode API endpoint for evaluator instance
  # Can be same as executor or different (e.g., different model/instance)
  api_endpoint: http://localhost:8081
  
  # Model configuration for evaluator
  model:
    # Model name/identifier
    name: claude-3-5-sonnet-20241022
    # Optional model parameters
    temperature: 0.3  # Lower temperature for more consistent evaluation
    max_tokens: 4096
  
  # Timeout for evaluator API calls (seconds)
  timeout: 60
  
  # Evaluation criteria weights (must sum to 1.0)
  criteria_weights:
    task_completion: 0.30    # Did the skill complete the task?
    rpc_correctness: 0.25    # Were correct RPC methods used?
    reasoning_quality: 0.20  # Was the reasoning logical?
    error_handling: 0.15     # Were errors handled well?
    response_quality: 0.10   # Was the response clear?
  
  # Pass threshold (0.0-1.0)
  pass_threshold: 0.7
  
  # Optional authentication for OpenCode API
  auth:
    type: none
    token: ""
    api_key_header: "X-API-Key"

# aria2 Mock Server Configuration
mock_server:
  # Default port for mock aria2 RPC server
  port: 6800
  # Host binding
  host: localhost
  # Verbose logging
  verbose: false

# Test Output Configuration
output:
  # Base directory for test results
  base_dir: ../results
  # Whether to generate all report formats
  generate_reports: true
  # Report formats to generate: json, yaml, text
  report_formats:
    - json
    - yaml
    - text
  # Pretty print JSON output
  pretty_json: true
  # Include detailed execution traces in reports
  include_traces: true

# Logging Configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: INFO
  # Log to file
  log_to_file: false
  # Log file path (if log_to_file is true)
  log_file: llm_eval.log
